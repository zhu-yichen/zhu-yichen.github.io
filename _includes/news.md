<h1 id="news"></h1>

<h2 style="margin: 30px 0px 10px;">News</h2>

<ul>
<li><strong>[Dec. 2024]</strong> We introduce a new CoT-like method <span style="color:#e74d3c">Chain-of-Affordance</span>, check out our project page <span style="color:#e74d3c"><a href="https://chain-of-affordance.github.io/">chain-of-affordance.github.io</a></span>.</li>
<li><strong>[Dec. 2024]</strong> We release a very strong robot foundation model called <span style="color:#e74d3c">Diffusion-VLA</span>, check out our project page <span style="color:#e74d3c"><a href="https://diffusion-vla.github.io/">diffusion-vla.github.io</a></span>.</li>
<li><strong>[Dec. 2024]</strong> Our paper Mipha, exploring the training strategy of multimodal small language model, is accepted to <span style="color:#e74d3c">AAAI 2025</span>.</li>
<li><strong>[Sep. 2024]</strong> Three papers are accepted to <span style="color:#e74d3c">NeurIPS 2024</span>.</li>
<li><strong>[Sep. 2024]</strong> We release <span style="color:#e74d3c">ScaleDP</span>, which scaling up the <span style="color:#e74d3c">Diffusion Policy to 1B parameters</span>. Project can be found at <span style="color:#e74d3c"><a href="https://scaling-diffusion-policy.github.io/">scaling-diffusion-policy.github.io</a></span>.</li>
<li><strong>[Sep. 2024]</strong> We present <span style="color:#e74d3c">TinyVLA</span>, a family of lightweight & data-efficient vision-language-action models. Project can be found at <span style="color:#e74d3c"><a href="https://tiny-vla.github.io/">tiny-vla.github.io</a></span>.</li>
</ul>
